{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATD-45w_oSMu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfCUFf5TGDR",
        "outputId": "f64e59e9-1ac7-4f95-a6e3-64b88df6319e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_CSC591'...\n",
            "remote: Enumerating objects: 1099, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 1099 (delta 39), reused 42 (delta 29), pack-reused 1037 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1099/1099), 21.34 MiB | 8.64 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/chiragbheemaiah/LPRNet_CSC591.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNnqDgZCWm1O",
        "outputId": "fcc6ef5d-83fc-4c68-9b21-5d81562e2355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_CSC591\n"
          ]
        }
      ],
      "source": [
        "cd LPRNet_CSC591"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws2Zo-LVonvO"
      },
      "source": [
        "## Baseline Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRdWn0M2WyEi",
        "outputId": "3b6936ac-fa5f-418a-aabe-2ec4dbb22db2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_CSC591/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.896 [896:63:41:1000]\n",
            "[Info] Test Speed: 0.18491456174850465s 1/1000]\n"
          ]
        }
      ],
      "source": [
        "! python test_LPRNet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzUikxzJoOsV"
      },
      "source": [
        "# Model Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oL1BDzsJX8U-"
      },
      "outputs": [],
      "source": [
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from model.LPRNet import build_lprnet\n",
        "# import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "import copy\n",
        "from types import SimpleNamespace\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SB9af3U_CNoY"
      },
      "outputs": [],
      "source": [
        "args = {\n",
        "    'img_size': [94, 24],\n",
        "    'test_img_dirs': \"./data/test\",\n",
        "    'dropout_rate': 0,\n",
        "    'lpr_max_len': 8,\n",
        "    'test_batch_size': 100,\n",
        "    'phase_train': False,\n",
        "    'num_workers': 8,\n",
        "    'cuda': False,\n",
        "    'show': False,\n",
        "    'pretrained_model': './weights/Final_LPRNet_model.pth'\n",
        "}\n",
        "\n",
        "args = SimpleNamespace(**args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_drwD0Xoq4x",
        "outputId": "687e20f4-9aef-4203-c43d-f02ecac55add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n"
          ]
        }
      ],
      "source": [
        "lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "lprnet.to(device)\n",
        "print(\"Successful to build network!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhQtv6jxVjIy",
        "outputId": "b7b43be0-8bee-449e-f5b6-c6e3b101facd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load pretrained model successful!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2409f5438df2>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
          ]
        }
      ],
      "source": [
        "# load pretrained model\n",
        "if args.pretrained_model:\n",
        "    lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
        "    print(\"load pretrained model successful!\")\n",
        "else:\n",
        "    print(\"[Error] Can't found pretrained mode, please check!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm-eJy501_ha"
      },
      "source": [
        "## Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVktjGvPnczh",
        "outputId": "b5c2b3da-0304-4968-e6a6-8a5b810d8e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LPRNet(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv2d(\n",
              "      3, 64, kernel_size=(3, 3), stride=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (1): BatchNorm2d(\n",
              "      64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(\n",
              "          64, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(\n",
              "          32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(\n",
              "          32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(\n",
              "          32, 128, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): BatchNorm2d(\n",
              "      128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(\n",
              "          64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(\n",
              "          64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(\n",
              "          64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(\n",
              "          64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): BatchNorm2d(\n",
              "      256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (10): ReLU()\n",
              "    (11): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(\n",
              "          256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(\n",
              "          64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(\n",
              "          64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(\n",
              "          64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
              "          (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): BatchNorm2d(\n",
              "      256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Dropout(p=0, inplace=False)\n",
              "    (16): Conv2d(\n",
              "      64, 256, kernel_size=(1, 4), stride=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (17): BatchNorm2d(\n",
              "      256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (18): ReLU()\n",
              "    (19): Dropout(p=0, inplace=False)\n",
              "    (20): Conv2d(\n",
              "      256, 68, kernel_size=(13, 1), stride=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (21): BatchNorm2d(\n",
              "      68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "    (22): ReLU()\n",
              "  )\n",
              "  (container): Sequential(\n",
              "    (0): Conv2d(\n",
              "      516, 68, kernel_size=(1, 1), stride=(1, 1)\n",
              "      (activation_post_process): HistogramObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "lprnet.eval()\n",
        "lprnet.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
        "lprnet_with_quant = torch.ao.quantization.prepare(lprnet)\n",
        "# Verify the structure\n",
        "lprnet_with_quant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMLOf2rDDfO1",
        "outputId": "7aee7aea-2aa1-48aa-82ba-e04ddca2c13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "lprnet_with_quant = torch.ao.quantization.convert(lprnet_with_quant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPF8fxwCDfa3",
        "outputId": "9c7cf2e5-03fb-4d69-c345-64be16a18e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check statistics of the various layers\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LPRNet(\n",
              "  (backbone): Sequential(\n",
              "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "    (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "        (1): ReLU()\n",
              "        (2): QuantizedConv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): QuantizedConv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): QuantizedConv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "      )\n",
              "    )\n",
              "    (5): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "        (1): ReLU()\n",
              "        (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "      )\n",
              "    )\n",
              "    (9): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "        (1): ReLU()\n",
              "        (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=1.0, zero_point=0, padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=1.0, zero_point=0, padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "      )\n",
              "    )\n",
              "    (12): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): QuantizedDropout(p=0, inplace=False)\n",
              "    (16): QuantizedConv2d(64, 256, kernel_size=(1, 4), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "    (17): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU()\n",
              "    (19): QuantizedDropout(p=0, inplace=False)\n",
              "    (20): QuantizedConv2d(256, 68, kernel_size=(13, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "    (21): QuantizedBatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "  )\n",
              "  (container): Sequential(\n",
              "    (0): QuantizedConv2d(516, 68, kernel_size=(1, 1), stride=(1, 1), scale=1.0, zero_point=0)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print(f'Check statistics of the various layers')\n",
        "lprnet_with_quant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSRuuK5CDfk_",
        "outputId": "4cf3e0dc-7148-4373-f8d8-7a12872ab3b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights after quantization\n",
            "tensor([[ 83, 127,  27],\n",
            "        [ 40,  27,  17],\n",
            "        [ 20,  37, -11]], dtype=torch.int8)\n"
          ]
        }
      ],
      "source": [
        "# Print the weights matrix of the model after quantization\n",
        "print('Weights after quantization')\n",
        "print(torch.int_repr(lprnet_with_quant.backbone[0].weight()[0][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PK4DcYvEzvZ",
        "outputId": "6f3e6e19-e186-421d-db52-e5f15e9ff890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original weights: \n",
            "tensor([[ 0.6627,  1.0154,  0.2187],\n",
            "        [ 0.3205,  0.2122,  0.1328],\n",
            "        [ 0.1622,  0.2951, -0.0841]], grad_fn=<SelectBackward0>)\n",
            "\n",
            "Dequantized weights: \n",
            "tensor([[ 0.6610,  1.0115,  0.2150],\n",
            "        [ 0.3186,  0.2150,  0.1354],\n",
            "        [ 0.1593,  0.2947, -0.0876]])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Original weights: ')\n",
        "print(lprnet.backbone[0].weight[0][0])\n",
        "print('')\n",
        "print(f'Dequantized weights: ')\n",
        "print(torch.dequantize(lprnet_with_quant.backbone[0].weight()[0][0]))\n",
        "print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "h4Kjj8G95eXK"
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
        "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
        "    os.remove('temp_delme.p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH1H89mtFk3X",
        "outputId": "bebc83cd-dee3-4d62-b491-5079ee7e3867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the model before quantization\n",
            "Size (KB): 1816.738\n",
            "Size of the model after quantization\n",
            "Size (KB): 533.576\n"
          ]
        }
      ],
      "source": [
        "print('Size of the model before quantization')\n",
        "print_size_of_model(lprnet)\n",
        "print('Size of the model after quantization')\n",
        "print_size_of_model(lprnet_with_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g363inswfbge"
      },
      "source": [
        "## Save Quantized Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r4Y2kL6GMRp1"
      },
      "outputs": [],
      "source": [
        "quant_weight_path = '/content/LPRNet_CSC591/weights/lprnet_quantized_weights.pth'\n",
        "torch.save(lprnet_with_quant.state_dict(), quant_weight_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cusKFu0-euLr"
      },
      "source": [
        "## Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T6WOzd6k-E3s"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XxHT7xju9Ah1"
      },
      "outputs": [],
      "source": [
        "def Greedy_Decode_Eval(Net, datasets, args):\n",
        "    # TestNet = Net.eval()\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        # load train data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start+length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "        # print(imgs.shape)\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        # forward\n",
        "        prebs = Net(images)\n",
        "        # greedy decode\n",
        "        prebs = prebs.cpu().detach().numpy()\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label: # dropout repeate label and blank label\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            # show image and its predict label\n",
        "            # if args.show:\n",
        "            #     show(imgs[i], label, targets[i])\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "En5eSUvo7mGo"
      },
      "outputs": [],
      "source": [
        "def test(model):\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "    Greedy_Decode_Eval(model, test_dataset, args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM_0prkafmuU"
      },
      "source": [
        "## Testing LPRNet Accuracy with DeQuantized Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4H8ZrX9fq2-",
        "outputId": "fd74f17e-0ba8-4c0d-d9ba-e216b76aa712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully built network!\n"
          ]
        }
      ],
      "source": [
        "lprnet_dequant = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "lprnet_dequant.to(device)\n",
        "print(\"Successfully built network!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2nRqvRjf-n5",
        "outputId": "cb695230-9ca6-4a60-ffc2-27ec68019cd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-19e500c89f83>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  quantized_weights_state_dict = torch.load(quant_weight_path)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:413: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  device=storage.device,\n"
          ]
        }
      ],
      "source": [
        "quantized_weights_state_dict = torch.load(quant_weight_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zVSidRAegjMS"
      },
      "outputs": [],
      "source": [
        "# Dequantize and load the weights into the model\n",
        "dequantized_state_dict = OrderedDict()\n",
        "for key, value in quantized_weights_state_dict.items():\n",
        "    if 'scale' in key or 'zero_point' in key:\n",
        "        continue\n",
        "    if value.is_quantized:\n",
        "        dequantized_state_dict[key] = value.dequantize()\n",
        "    else:\n",
        "        dequantized_state_dict[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0u7_A3Ng51b",
        "outputId": "0bcfa9c7-f764-4c7d-cef4-83554630732d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "lprnet_dequant.load_state_dict(dequantized_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPx1mWVAGDIF",
        "outputId": "c02d882f-b078-4975-c37f-1e1f561773cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the model after quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.9 [900:59:41:1000]\n",
            "[Info] Test Speed: 0.030635921716690065s 1/1000]\n"
          ]
        }
      ],
      "source": [
        "print('Testing the model after quantization')\n",
        "test(lprnet_dequant)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}