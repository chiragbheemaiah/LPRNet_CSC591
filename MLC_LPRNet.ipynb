{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y7VyRi_pk0o_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "eq51ApthmXAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m  pip install mlc-ai-cpu -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYS-TPdVO80L",
        "outputId": "eab08639-d7c6-4d6f-e170-53d2facbb603"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-cpu\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_cpu-0.17.2-cp310-cp310-manylinux_2_28_x86_64.whl (185.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.12.2)\n",
            "Installing collected packages: mlc-ai-cpu\n",
            "Successfully installed mlc-ai-cpu-0.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QPtwkVrnwWX",
        "outputId": "2ef5b5e7-ddad-438d-aec1-a879fdb329c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_CSC591'...\n",
            "remote: Enumerating objects: 1087, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1087 (delta 33), reused 34 (delta 25), pack-reused 1037 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1087/1087), 20.46 MiB | 15.84 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone -b dev_chirag https://github.com/chiragbheemaiah/LPRNet_CSC591.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd LPRNet_CSC591"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8pa3QARog1c",
        "outputId": "b6c8b263-e34e-4f0a-b279-024173d850fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LPRNet_CSC591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "MH8egR-duw5Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JIT Trace NN Module"
      ],
      "metadata": {
        "id": "hdKb4FhroO_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class LPRNet(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1), # 0\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),  # 2\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),    # *** 4 ***\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),  # 6\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),   # 8\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 10\n",
        "            small_basic_block(ch_in=256, ch_out=256),   # *** 11 ***\n",
        "            nn.BatchNorm2d(num_features=256),   # 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),  # 14\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),  # 16\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 18\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1), # 20\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),  # *** 22 ***\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]: # [2, 4, 8, 11, 22]\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def build_lprnet(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "\n",
        "    Net = LPRNet(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return Net.train()\n",
        "    else:\n",
        "        return Net.eval()"
      ],
      "metadata": {
        "id": "OR1tpbhqhlk-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "lprnet.load_state_dict(torch.load(\"./weights/Final_LPRNet_model.pth\",  map_location=torch.device('cpu')))\n",
        "lprnet.eval()\n",
        "\n",
        "example_input = torch.randn(1, 3, 24, 94)\n",
        "traced_model = torch.jit.trace(lprnet, example_input)\n",
        "traced_model.save(\"lprnet_traced.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCITtvasjYa1",
        "outputId": "28a1b7ea-cb2e-43a6-fe27-b660312de677"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-0c7754a45614>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(\"./weights/Final_LPRNet_model.pth\",  map_location=torch.device('cpu')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import relay\n",
        "from tvm.contrib.download import download_testdata\n",
        "\n",
        "# Load the TorchScript model\n",
        "scripted_model = torch.jit.load(\"lprnet_traced.pt\")\n",
        "scripted_model.eval()"
      ],
      "metadata": {
        "id": "QMH5ovBclK9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile into Relay Module"
      ],
      "metadata": {
        "id": "5h7u1xf-WZW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "args = {\n",
        "    'img_size': [94, 24],\n",
        "    'test_img_dirs': \"./data/test\",\n",
        "    'dropout_rate': 0,\n",
        "    'lpr_max_len': 8,\n",
        "    'test_batch_size': 100,\n",
        "    'phase_train': False,\n",
        "    'num_workers': 8,\n",
        "    'cuda': False,\n",
        "    'show': False,\n",
        "    'pretrained_model': './weights/Final_LPRNet_model.pth'\n",
        "}\n",
        "\n",
        "args = SimpleNamespace(**args)"
      ],
      "metadata": {
        "id": "uFT97ExjQMEY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (args.test_batch_size, 3, 24, 94)\n",
        "input_name = \"input0\"\n",
        "input_shapes = [(input_name, input_shape)]\n",
        "\n",
        "# Convert to TVM Relay format\n",
        "mod, params = relay.frontend.from_pytorch(scripted_model, input_shapes)"
      ],
      "metadata": {
        "id": "0pZV6jYCWP2a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target device\n",
        "target = \"llvm\"\n",
        "dev = tvm.cuda(0) if target == \"cuda\" else tvm.cpu()\n",
        "\n",
        "# Compile the model\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target=target, params=params)\n"
      ],
      "metadata": {
        "id": "VgoUDAW-msFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101dc5ac-3a28-4065-d569-22dbac5155f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.contrib import graph_executor\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))"
      ],
      "metadata": {
        "id": "ZeoGJHmGm_gD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Function"
      ],
      "metadata": {
        "id": "t6gjVOqMPZXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from model.LPRNet import build_lprnet\n",
        "# import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
        "\n",
        "def test(module):\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "    Greedy_Decode_Eval(module, test_dataset, args)\n",
        "\n",
        "def Greedy_Decode_Eval(module, datasets, args):\n",
        "    # TestNet = Net.eval()\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        # load train data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start+length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        # forward\n",
        "        # prebs = Net(images)\n",
        "        # Set input and run\n",
        "        module.set_input(input_name, tvm.nd.array(images.numpy()))\n",
        "        module.run()\n",
        "\n",
        "        # Get output\n",
        "        tvm_output = module.get_output(0).asnumpy()\n",
        "        print(\"Output shape:\", tvm_output.shape)\n",
        "        prebs = tvm_output\n",
        "        # greedy decode\n",
        "        # prebs = prebs.cpu().detach().numpy()\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label: # dropout repeate label and blank label\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            # show image and its predict label\n",
        "            # if args.show:\n",
        "                # show(imgs[i], label, targets[i])\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
      ],
      "metadata": {
        "id": "3L5huP8aPT_r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual Optimization"
      ],
      "metadata": {
        "id": "WmKhkVMoM6QX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic manual optimization passes\n",
        "# 1. Canonicalization and Simplification\n",
        "mod_manual = relay.transform.InferType()(mod)\n",
        "mod_manual = relay.transform.SimplifyInference()(mod_manual)\n",
        "mod_manual = relay.transform.CanonicalizeOps()(mod_manual)\n",
        "\n",
        "# 2. Basic Arithmetic Simplification\n",
        "mod_manual = relay.transform.FoldConstant()(mod_manual)\n",
        "mod_manual = relay.transform.CombineParallelConv2D()(mod_manual)\n",
        "\n",
        "# 3. Layout Transformation (if applicable)\n",
        "# This can help optimize convolution and other spatial operations\n",
        "mod_manual = relay.transform.AlterOpLayout()(mod_manual)\n",
        "\n",
        "# 4. Dead Code Elimination\n",
        "mod_manual = relay.transform.DeadCodeElimination()(mod_manual)\n",
        "\n",
        "# 5. Optimize memory usage\n",
        "mod_manual = relay.transform.EliminateCommonSubexpr()(mod_manual)\n",
        "\n",
        "# # 6. Advanced Optimization Passes\n",
        "# # These can further optimize the computation graph\n",
        "# mod_nn = relay.transform.CombineParallelDense()(mod_nn)\n",
        "# mod_nn = relay.transform.PlanDevices()(mod_nn)"
      ],
      "metadata": {
        "id": "YLO2a_FpuHZ-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target device\n",
        "target = \"llvm\"\n",
        "dev = tvm.cuda(0) if target == \"cuda\" else tvm.cpu()\n",
        "\n",
        "# Compile the model\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod_manual, target=target, params=params)"
      ],
      "metadata": {
        "id": "weHudhLHjKRR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.contrib import graph_executor\n",
        "\n",
        "# Create a graph executor\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "\n"
      ],
      "metadata": {
        "id": "mhgk40BLjKRS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "132CjOU1PUFE",
        "outputId": "18ec3182-a1d2-453b-e9b5-b1320c91cf64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "[Info] Test Accuracy: 0.898 [898:61:41:1000]\n",
            "[Info] Test Speed: 0.046563982248306275s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto Tuning"
      ],
      "metadata": {
        "id": "Ry1fhoNC_rD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm.auto_scheduler as auto_scheduler\n",
        "from tvm.autotvm.tuner import XGBTuner\n",
        "from tvm import autotvm"
      ],
      "metadata": {
        "id": "8XYN7di7_Vpv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number = 10\n",
        "repeat = 1\n",
        "min_repeat_ms = 0  # since we're tuning on a CPU, can be set to 0\n",
        "timeout = 10  # in seconds\n",
        "\n",
        "# create a TVM runner\n",
        "runner = autotvm.LocalRunner(\n",
        "    number=number,\n",
        "    repeat=repeat,\n",
        "    timeout=timeout,\n",
        "    min_repeat_ms=min_repeat_ms,\n",
        "    enable_cpu_cache_flush=True,\n",
        ")"
      ],
      "metadata": {
        "id": "E2mUjQlt_Vx2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuning_option = {\n",
        "    \"tuner\": \"xgb\",\n",
        "    \"trials\": 20,\n",
        "    \"early_stopping\": 100,\n",
        "    \"measure_option\": autotvm.measure_option(\n",
        "        builder=autotvm.LocalBuilder(build_func=\"default\"), runner=runner\n",
        "    ),\n",
        "    \"tuning_records\": \"lprnet-autotuning.json\",\n",
        "}"
      ],
      "metadata": {
        "id": "r9t3lC1a_0Re"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin by extracting the tasks from the onnx model\n",
        "tasks = autotvm.task.extract_from_program(mod[\"main\"], target=target, params=params)\n",
        "# print(tasks)\n",
        "# Tune the extracted tasks sequentially.\n",
        "for i, task in enumerate(tasks):\n",
        "    prefix = \"[Task %2d/%2d] \" % (i + 1, len(tasks))\n",
        "\n",
        "    # choose tuner\n",
        "    tuner = \"xgb\"\n",
        "\n",
        "    # create tuner\n",
        "    if tuner == \"xgb\":\n",
        "        tuner_obj = XGBTuner(task, loss_type=\"reg\")\n",
        "    # elif tuner == \"xgb_knob\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"knob\")\n",
        "    # elif tuner == \"xgb_itervar\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"itervar\")\n",
        "    # elif tuner == \"xgb_curve\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"reg\", feature_type=\"curve\")\n",
        "    # elif tuner == \"xgb_rank\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank\")\n",
        "    # elif tuner == \"xgb_rank_knob\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"knob\")\n",
        "    # elif tuner == \"xgb_rank_itervar\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"itervar\")\n",
        "    # elif tuner == \"xgb_rank_curve\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank\", feature_type=\"curve\")\n",
        "    # elif tuner == \"xgb_rank_binary\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank-binary\")\n",
        "    # elif tuner == \"xgb_rank_binary_knob\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"knob\")\n",
        "    # elif tuner == \"xgb_rank_binary_itervar\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"itervar\")\n",
        "    # elif tuner == \"xgb_rank_binary_curve\":\n",
        "    #     tuner_obj = XGBTuner(task, loss_type=\"rank-binary\", feature_type=\"curve\")\n",
        "    # elif tuner == \"ga\":\n",
        "    #     tuner_obj = GATuner(task, pop_size=50)\n",
        "    # elif tuner == \"random\":\n",
        "    #     tuner_obj = RandomTuner(task)\n",
        "    # elif tuner == \"gridsearch\":\n",
        "    #     tuner_obj = GridSearchTuner(task)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid tuner: \" + tuner)\n",
        "\n",
        "    tuner_obj.tune(\n",
        "        n_trial=min(tuning_option[\"trials\"], len(task.config_space)),\n",
        "        early_stopping=tuning_option[\"early_stopping\"],\n",
        "        measure_option=tuning_option[\"measure_option\"],\n",
        "        callbacks=[\n",
        "            autotvm.callback.progress_bar(tuning_option[\"trials\"], prefix=prefix),\n",
        "            autotvm.callback.log_to_file(tuning_option[\"tuning_records\"]),\n",
        "        ],\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuFXXkhp_0WW",
        "outputId": "2ea3551c-f598-4c19-bb2d-b3a5a6656e0c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Task  1/13]  Current/Best:    7.57/  14.67 GFLOPS | Progress: (20/20) | 59.73 s Done.\n",
            "[Task  2/13]  Current/Best:    5.85/  16.11 GFLOPS | Progress: (20/20) | 56.82 s Done.\n",
            "[Task  3/13]  Current/Best:   11.94/  12.54 GFLOPS | Progress: (20/20) | 75.65 s Done.\n",
            "[Task  4/13]  Current/Best:   17.76/  17.76 GFLOPS | Progress: (20/20) | 56.15 s Done.\n",
            "[Task  5/13]  Current/Best:    6.73/  15.62 GFLOPS | Progress: (20/20) | 88.26 s Done.\n",
            "[Task  6/13]  Current/Best:   13.15/  14.57 GFLOPS | Progress: (20/20) | 48.68 s Done.\n",
            "[Task  7/13]  Current/Best:    4.97/  14.86 GFLOPS | Progress: (20/20) | 104.96 s Done.\n",
            "[Task  8/13]  Current/Best:    9.69/  17.46 GFLOPS | Progress: (20/20) | 89.51 s Done.\n",
            "[Task  9/13]  Current/Best:   14.74/  15.28 GFLOPS | Progress: (20/20) | 133.46 s Done.\n",
            "[Task 10/13]  Current/Best:    5.09/  12.42 GFLOPS | Progress: (20/20) | 133.30 s Done.\n",
            "[Task 11/13]  Current/Best:   16.12/  16.70 GFLOPS | Progress: (20/20) | 132.83 s Done.\n",
            "[Task 12/13]  Current/Best:    5.45/  10.41 GFLOPS | Progress: (20/20) | 184.16 s Done.\n",
            "[Task 13/13]  Current/Best:    2.72/  12.17 GFLOPS | Progress: (20/20) | 53.15 s Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with autotvm.apply_history_best(tuning_option[\"tuning_records\"]):\n",
        "    with tvm.transform.PassContext(opt_level=3, config={}):\n",
        "        lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "dev = tvm.device(str(target), 0)\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))"
      ],
      "metadata": {
        "id": "oWLzTmY9_0bM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjwjEhaUmSKZ",
        "outputId": "3469da79-f0ff-4c75-d669-9c2a43a4d20a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "Output shape: (100, 68, 18)\n",
            "[Info] Test Accuracy: 0.901 [901:57:42:1000]\n",
            "[Info] Test Speed: 0.03988617014884949s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Performance"
      ],
      "metadata": {
        "id": "KMhzig4GmLZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python /content/LPRNet_CSC591/test_LPRNet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwVEXkKCh7uH",
        "outputId": "851409fe-e63b-40d8-d648-3178a4700a66"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_CSC591/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.896 [896:62:42:1000]\n",
            "[Info] Test Speed: 0.22646905851364135s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the module (includes weights/parameters)\n",
        "module_path = \"module.tar\"\n",
        "lib.export_library(module_path)\n",
        "print(f\"Module saved to {module_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iPV7cSKzu93",
        "outputId": "23550742-661a-42a7-e87e-57092fc1dea0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Module saved to module.tar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Function - Single Image"
      ],
      "metadata": {
        "id": "EwpvDhnLhuLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARS = ['京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
        "         '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
        "         '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
        "         '新',\n",
        "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
        "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
        "         'W', 'X', 'Y', 'Z', 'I', 'O', '-'\n",
        "         ]\n",
        "\n",
        "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
        "len(CHARS)"
      ],
      "metadata": {
        "id": "QbUwh2wIruYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import functional as TF\n",
        "\n",
        "def preprocess_image(image_path, img_size=(94, 24)):\n",
        "    # Load and resize the image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(img_size)  # Resize to target size\n",
        "    img = np.array(img).astype('float32')  # Convert to numpy array\n",
        "    img -= 127.5  # Normalize\n",
        "    img *= 0.0078125\n",
        "    img = np.transpose(img, (2, 0, 1))  # Convert HWC -> CHW\n",
        "    return torch.tensor(img).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "def test_single_image(model, image_tensor):\n",
        "    \"\"\"\n",
        "    Test a single image with the TVM-built model and decode the output.\n",
        "    Args:\n",
        "        model: The compiled TVM model.\n",
        "        image_tensor (torch.Tensor): Preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    # Forward pass\n",
        "    prebs = model(image_tensor)  # Pass the image tensor through the model\n",
        "    prebs = prebs.cpu().detach().numpy()  # Convert the output to NumPy array\n",
        "\n",
        "    preb_labels = []  # Initialize list to store final decoded labels\n",
        "    preb = prebs[0]  # Extract predictions for the single image in the batch\n",
        "\n",
        "    # Greedy decoding: Iterate over each time-step (over sequence length)\n",
        "    preb_label = []\n",
        "    for j in range(preb.shape[1]):  # Iterate over sequence positions (time steps)\n",
        "        preb_label.append(np.argmax(preb[:, j], axis=0))  # Get the most likely class at each position\n",
        "\n",
        "    # Remove repeated labels and blank labels\n",
        "    no_repeat_blank_label = []\n",
        "    pre_c = -1  # Initialize previous character as -1 (no character)\n",
        "\n",
        "    for c in preb_label:\n",
        "        if c == len(CHARS) - 1:  # Blank character (no prediction)\n",
        "            continue\n",
        "        if c == pre_c:  # Skip repeated labels\n",
        "            continue\n",
        "        no_repeat_blank_label.append(c)\n",
        "        pre_c = c  # Update previous character\n",
        "\n",
        "    # Decode characters from the predicted label indices\n",
        "    predicted_text = ''.join([CHARS[c] for c in no_repeat_blank_label])  # Convert indices to characters\n",
        "    print(\"Predicted Labels (Indices):\", no_repeat_blank_label)\n",
        "    print(\"Predicted Text:\", predicted_text)\n",
        "\n",
        "# Example usage:\n",
        "image_path = \"/content/LPRNet_CSC591/data/test/沪AMS087.jpg\"\n",
        "# model = torch.jit.load(\"path_to_tvm_model.pt\")  # Replace with your TVM-compiled model\n",
        "image_tensor = preprocess_image(image_path)\n",
        "# print(image_tensor.shape)\n",
        "test_single_image(model, image_tensor)\n"
      ],
      "metadata": {
        "id": "BVUQnknasE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ansiCJGOPUVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set input and run - ignore\n",
        "module.set_input(input_name, tvm.nd.array(input_data.numpy()))\n",
        "module.run()\n",
        "\n",
        "# Get output\n",
        "tvm_output = module.get_output(0).asnumpy()\n",
        "print(\"Output shape:\", tvm_output.shape)"
      ],
      "metadata": {
        "id": "BwLK0uGXD0dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ignore\n",
        "prebs = tvm_output\n",
        "preb_labels = []  # Initialize list to store final decoded labels\n",
        "preb = prebs[0]  # Extract predictions for the single image in the batch\n",
        "\n",
        "# Greedy decoding: Iterate over each time-step (over sequence length)\n",
        "preb_label = []\n",
        "for j in range(preb.shape[1]):  # Iterate over sequence positions (time steps)\n",
        "    preb_label.append(np.argmax(preb[:, j], axis=0))  # Get the most likely class at each position\n",
        "\n",
        "# Remove repeated labels and blank labels\n",
        "no_repeat_blank_label = []\n",
        "pre_c = -1  # Initialize previous character as -1 (no character)\n",
        "\n",
        "for c in preb_label:\n",
        "    if c == len(CHARS) - 1:  # Blank character (no prediction)\n",
        "        continue\n",
        "    if c == pre_c:  # Skip repeated labels\n",
        "        continue\n",
        "    no_repeat_blank_label.append(c)\n",
        "    pre_c = c  # Update previous character\n",
        "\n",
        "# Decode characters from the predicted label indices\n",
        "predicted_text = ''.join([CHARS[c] for c in no_repeat_blank_label])  # Convert indices to characters\n",
        "print(\"Predicted Labels (Indices):\", no_repeat_blank_label)\n",
        "print(\"Predicted Text:\", predicted_text)"
      ],
      "metadata": {
        "id": "xAEygtDDD0da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "timing_number = 10\n",
        "timing_repeat = 10\n",
        "optimized = (\n",
        "    np.array(timeit.Timer(lambda: module.run()).repeat(repeat=timing_repeat, number=timing_number))\n",
        "    * 1000\n",
        "    / timing_number\n",
        ")\n",
        "optimized = {\"mean\": np.mean(optimized), \"median\": np.median(optimized), \"std\": np.std(optimized)}\n",
        "\n",
        "\n",
        "print(\"optimized: %s\" % (optimized))\n",
        "print(\"unoptimized: %s\" % (unoptimized))"
      ],
      "metadata": {
        "id": "unNzNXRpD-wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set input and run\n",
        "module.set_input(input_name, tvm.nd.array(input_data.numpy()))\n",
        "module.run()\n",
        "\n",
        "# Get output\n",
        "tvm_output = module.get_output(0).asnumpy()\n",
        "print(\"Output shape:\", tvm_output.shape)"
      ],
      "metadata": {
        "id": "m39xD64G-97L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prebs = tvm_output\n",
        "preb_labels = []  # Initialize list to store final decoded labels\n",
        "preb = prebs[0]  # Extract predictions for the single image in the batch\n",
        "\n",
        "# Greedy decoding: Iterate over each time-step (over sequence length)\n",
        "preb_label = []\n",
        "for j in range(preb.shape[1]):  # Iterate over sequence positions (time steps)\n",
        "    preb_label.append(np.argmax(preb[:, j], axis=0))  # Get the most likely class at each position\n",
        "\n",
        "# Remove repeated labels and blank labels\n",
        "no_repeat_blank_label = []\n",
        "pre_c = -1  # Initialize previous character as -1 (no character)\n",
        "\n",
        "for c in preb_label:\n",
        "    if c == len(CHARS) - 1:  # Blank character (no prediction)\n",
        "        continue\n",
        "    if c == pre_c:  # Skip repeated labels\n",
        "        continue\n",
        "    no_repeat_blank_label.append(c)\n",
        "    pre_c = c  # Update previous character\n",
        "\n",
        "# Decode characters from the predicted label indices\n",
        "predicted_text = ''.join([CHARS[c] for c in no_repeat_blank_label])  # Convert indices to characters\n",
        "print(\"Predicted Labels (Indices):\", no_repeat_blank_label)\n",
        "print(\"Predicted Text:\", predicted_text)"
      ],
      "metadata": {
        "id": "NwzyH9bpoDF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wait on this"
      ],
      "metadata": {
        "id": "y7VyRi_pk0o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import tvm\n",
        "import tvm.relay as relay\n",
        "\n",
        "def export_to_tvm(model, input_shape=(1, 3, 24, 94)):\n",
        "    \"\"\"\n",
        "    Export PyTorch model to TVM IRModule\n",
        "\n",
        "    Args:\n",
        "    - model (nn.Module): Loaded PyTorch model\n",
        "    - input_shape (tuple): Expected input shape\n",
        "\n",
        "    Returns:\n",
        "    - TVM IRModule\n",
        "    - TVM input params\n",
        "    \"\"\"\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create a dummy input\n",
        "    dummy_input = torch.randn(input_shape)\n",
        "\n",
        "    try:\n",
        "        # Use torch.jit.script to create a scriptable model\n",
        "        scripted_model = torch.jit.script(model)\n",
        "\n",
        "        # Export to TVM Relay\n",
        "        input_info = [('input', input_shape)]\n",
        "        mod, params = relay.frontend.from_pytorch(scripted_model, input_info)\n",
        "\n",
        "        return mod, params\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during TVM export: {e}\")\n",
        "        raise\n",
        "\n",
        "def compile_tvm_model(mod, params, target='llvm', opt_level=3):\n",
        "    \"\"\"\n",
        "    Compile TVM IRModule\n",
        "\n",
        "    Args:\n",
        "    - mod (tvm.IRModule): TVM computational graph\n",
        "    - params (dict): Model parameters\n",
        "    - target (str): Compilation target\n",
        "    - opt_level (int): Optimization level\n",
        "\n",
        "    Returns:\n",
        "    - Compiled TVM runtime module\n",
        "    \"\"\"\n",
        "    # Set compilation target\n",
        "    target = tvm.target.Target(target)\n",
        "\n",
        "    # Create execution context\n",
        "    with tvm.transform.PassContext(opt_level=opt_level):\n",
        "        # Compile the model\n",
        "        executor = relay.build(mod, target, params=params)\n",
        "\n",
        "    return executor\n",
        "\n",
        "def main():\n",
        "    # Create the model\n",
        "    model = build_lprnet(phase='test')\n",
        "\n",
        "    # Path to your pre-trained weights (if needed)\n",
        "    # model.load_state_dict(torch.load('path/to/weights.pth'))\n",
        "\n",
        "    # Input shape (batch, channels, height, width)\n",
        "    input_shape = (1, 3, 24, 94)\n",
        "\n",
        "    try:\n",
        "        # Export to TVM IRModule\n",
        "        mod, params = export_to_tvm(model, input_shape)\n",
        "\n",
        "        # Compile the model\n",
        "        compiled_model = compile_tvm_model(mod, params)\n",
        "\n",
        "        # Optionally save the compiled model\n",
        "        compiled_model.export_library('lprnet_tvm.so')\n",
        "\n",
        "        print(\"Model successfully exported to TVM!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in TVM export process: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "ppRzrfuKjjDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i3CXpxiXkOVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tvm\n",
        "# from tvm import relay\n",
        "# from tvm import te\n",
        "# # from tvm import strategy  # Remove this incorrect import\n",
        "\n",
        "# # Import from the correct namespace:\n",
        "# # from tvm.tir.schedule import strategy\n",
        "\n",
        "# # Register a strategy for nn.dropout\n",
        "# # @relay.op.strategy..register(\"nn.dropout\")\n",
        "# # def dropout_strategy(attrs, inputs, out_type, target):\n",
        "# #     # Dropout is a no-op in inference\n",
        "# #     return inputs\n",
        "\n",
        "# # Your existing code follows:\n",
        "# @tvm.tir.transform.prim_func_pass(opt_level=0)\n",
        "# def print_tir(f, mod, ctx):\n",
        "#     print(f)\n",
        "#     return f\n",
        "\n",
        "# target = \"llvm\"\n",
        "# dev = tvm.cuda(0) if target == \"cuda\" else tvm.cpu()\n",
        "\n",
        "# with tvm.transform.PassContext(opt_level=3, config={\"tir.add_lower_pass\": [(3, print_tir)]}):\n",
        "#     mod_nn = relay.transform.InferType()(mod_nn)  # Infer type of the Relay model\n",
        "#     lowered = relay.backend.te_compiler.lower_to_primfunc(mod_nn[\"main\"], target=tvm.target.Target(\"llvm\"))\n",
        "#     print(lowered)"
      ],
      "metadata": {
        "id": "nualMAkKSmIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tvm.autotvm.record import load_from_file\n",
        "\n",
        "# log_file = tuning_option[\"tuning_records\"]\n",
        "\n",
        "# # # Print all tuning records\n",
        "# # for record in load_from_file(log_file):\n",
        "# #     print(record)\n",
        "# # from tvm.autotvm.record import apply_history_best\n",
        "\n",
        "# with autotvm.apply_history_best(log_file):\n",
        "#     best_config = task.config_space.get(task.best_config_index)\n",
        "#     print(best_config)"
      ],
      "metadata": {
        "id": "2oWGX-YvEnRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_file = tuning_option[\"tuning_records\"]\n",
        "\n",
        "# with autotvm.apply_history_best(log_file):\n",
        "with tvm.target.Target(target):\n",
        "    s, arg_bufs = task.func(*task.args)\n",
        "    print(tvm.lower(s, arg_bufs, simple_mode=True))  # Print the lowered IR\n"
      ],
      "metadata": {
        "id": "dmT85jpjFpGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}